{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AudioLSTM.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWrHl8vQKFlu",
        "colab_type": "text"
      },
      "source": [
        "##Loading all library required"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gEfre8MdhB9-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torchvision\n",
        "from skimage.io import imread\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.metrics import accuracy_score\n",
        "import torch.optim as optim\n",
        "import os\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "import torch.utils.data as data\n",
        "import torchvision\n",
        "from torch.autograd import Variable\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import pickle\n",
        "import pandas as pd\n",
        "from skimage.io import imread\n",
        "from skimage import io, transform\n",
        "from PIL import Image "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1lg0OWodp0b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# EncoderCNN architecture\n",
        "CNN_fc_hidden1, CNN_fc_hidden2 = 1024, 768\n",
        "CNN_embed_dim = 512   # latent dim extracted by 2D CNN\n",
        "res_size = 224        # ResNet image size\n",
        "dropout_p = 0.3       # dropout probability\n",
        "\n",
        "# DecoderRNN architecture\n",
        "RNN_hidden_layers = 3\n",
        "RNN_hidden_nodes = 512\n",
        "RNN_FC_dim = 256\n",
        "\n",
        "# training parameters\n",
        "k = 5             # number of target category\n",
        "epochs = 10        # training epochs\n",
        "batch_size = 40  \n",
        "learning_rate = 1e-3\n",
        "log_interval = 10   # interval for displaying training info\n",
        "\n",
        "# Select which frame to begin & end in videos\n",
        "begin_frame, end_frame, skip_frame = 1, 29, 1\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "295HI5PhdYYk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from model import *"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNsr_W6kKKsn",
        "colab_type": "text"
      },
      "source": [
        "###Loading model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tf0vuEFFhX9s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Flatten(nn.Module):\n",
        "    def forward(self, input):\n",
        "        return input.view(input.size(0), -1)\n",
        "      \n",
        "class MyResnet(nn.Module):\n",
        "    def __init__(self, inp = 2048, h1=1024, out = 5, d=0.30):\n",
        "        super().__init__()\n",
        "        resnet = torchvision.models.resnet50()\n",
        "        modules = list(resnet.children())[:-2]\n",
        "        self.resnet = nn.Sequential(*modules)\n",
        "        self.ap = nn.AdaptiveAvgPool2d((1,1))\n",
        "        self.mp = nn.AdaptiveMaxPool2d((1,1))\n",
        "        self.fla = Flatten()\n",
        "        self.bn0 = nn.BatchNorm1d(inp*2,eps=1e-05, momentum=0.1, affine=True)\n",
        "        self.dropout0 = nn.Dropout(d)\n",
        "        self.fc1 = nn.Linear(inp*2, h1)\n",
        "        self.bn1 = nn.BatchNorm1d(h1,eps=1e-05, momentum=0.1, affine=True)\n",
        "        self.dropout1 = nn.Dropout(d)\n",
        "        self.fc2 = nn.Linear(h1, out)\n",
        "        for m in self.modules():\n",
        "          if isinstance(m,nn.Linear):\n",
        "            nn.init.kaiming_normal_(m.weight)\n",
        "            nn.init.zeros_(m.bias)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.resnet(x)\n",
        "        ap = self.ap(x)\n",
        "        mp = self.mp(x)\n",
        "        x = torch.cat((ap,mp),dim=1)\n",
        "        x = self.fla(x)\n",
        "        x = self.bn0(x)\n",
        "        x = self.dropout0(x)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.bn1(x)\n",
        "        x = self.dropout1(x)         \n",
        "        x = torch.sigmoid_(self.fc2(x))\n",
        "        \n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZBn7E09hWbD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, CNN_embed_dim=300, h_RNN_layers=3, h_RNN=256, h_FC_dim=128, drop_p=0.3, num_classes=5):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "\n",
        "        self.RNN_input_size = CNN_embed_dim\n",
        "        self.h_RNN_layers = h_RNN_layers   # RNN hidden layers\n",
        "        self.h_RNN = h_RNN                 # RNN hidden nodes\n",
        "        self.h_FC_dim = h_FC_dim\n",
        "        self.drop_p = drop_p\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        self.LSTM = nn.LSTM(\n",
        "            input_size=self.RNN_input_size,\n",
        "            hidden_size=self.h_RNN,        \n",
        "            num_layers=h_RNN_layers,       \n",
        "            batch_first=True,       # input & output will has batch size as 1s dimension. e.g. (batch, time_step, input_size)\n",
        "        )\n",
        "\n",
        "        self.fc1 = nn.Linear(self.h_RNN, self.h_FC_dim)\n",
        "        self.fc2 = nn.Linear(self.h_FC_dim, self.num_classes)\n",
        "\n",
        "    def forward(self, x_RNN):\n",
        "        \n",
        "        self.LSTM.flatten_parameters()\n",
        "        RNN_out, (h_n, h_c) = self.LSTM(x_RNN, None)  \n",
        "        \"\"\" h_n shape (n_layers, batch, hidden_size), h_c shape (n_layers, batch, hidden_size) \"\"\" \n",
        "        \"\"\" None represents zero initial hidden state. RNN_out has shape=(batch, time_step, output_size) \"\"\"\n",
        "        X=[]\n",
        "        X2=[]\n",
        "\n",
        "        for t in range(RNN_out.size(1)):\n",
        "\n",
        "          x1=RNN_out[:,t,:]\n",
        "          x1 = self.fc1(x1)   # choose RNN_out at the last time step\n",
        "          x1 = F.relu(x1)\n",
        "          x1 = F.dropout(x1, p=self.drop_p, training=self.training)\n",
        "          x1 = self.fc2(x1)\n",
        "          x1 = torch.sigmoid(x1)\n",
        "          X.append(x1)\n",
        "        x3=torch.mean(torch.stack(X),dim=0)\n",
        "\n",
        "\n",
        "        return x3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCWsJ13HhZOk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ResCNNEncoder(nn.Module):\n",
        "    def __init__(self,modelA, fc_hidden1=512, fc_hidden2=512, drop_p=0.3, CNN_embed_dim=300):\n",
        "        \"\"\"Load the pretrained ResNet-152 and replace top fc layer.\"\"\"\n",
        "        super(ResCNNEncoder, self).__init__()\n",
        "        modules=list(modelA.children())[:-9]\n",
        "        self.model=nn.Sequential(*modules)\n",
        "        self.ap = nn.AdaptiveAvgPool2d((1,1))\n",
        "        self.mp = nn.AdaptiveMaxPool2d((1,1))\n",
        "        self.fla = Flatten()\n",
        "        self.fc_hidden1, self.fc_hidden2 = fc_hidden1, fc_hidden2\n",
        "        self.drop_p = drop_p\n",
        "        self.fc1 = nn.Linear(2048*2, fc_hidden1)\n",
        "        self.bn1 = nn.BatchNorm1d(fc_hidden1, momentum=0.01)\n",
        "        self.fc2 = nn.Linear(fc_hidden1, fc_hidden2)\n",
        "        self.bn2 = nn.BatchNorm1d(fc_hidden2, momentum=0.01)\n",
        "        self.fc3 = nn.Linear(fc_hidden2, CNN_embed_dim)\n",
        "        \n",
        "    def forward(self, x_3d):\n",
        "        cnn_embed_seq = []\n",
        "        for t in range(x_3d.size(1)):\n",
        "            with torch.no_grad():\n",
        "                images=x_3d[:,t, :, :, :]\n",
        "                images = images.view(-1,3, 224, 224)  \n",
        "                x = self.model(images.type(torch.cuda.FloatTensor))  \n",
        "            # FC layers\n",
        "            ap = self.ap(x)\n",
        "            mp = self.mp(x)\n",
        "            x = torch.cat((ap,mp),dim=1)\n",
        "            x = self.fla(x)\n",
        "            x = self.bn1(self.fc1(x))\n",
        "            x = F.relu(x)\n",
        "            x = self.bn2(self.fc2(x))\n",
        "            x = F.relu(x)\n",
        "            x = F.dropout(x, p=self.drop_p, training=self.training)\n",
        "            x = self.fc3(x)\n",
        "\n",
        "            cnn_embed_seq.append(x)\n",
        "\n",
        "        # swap time and sample dim such that (sample dim, time dim, CNN latent dim)\n",
        "        cnn_embed_seq = torch.stack(cnn_embed_seq, dim=0).transpose_(0, 1)\n",
        "        # cnn_embed_seq: shape=(batch, time_step, input_size)\n",
        "\n",
        "        return cnn_embed_seq"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "noLS1gX5haUU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Audio(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        \n",
        "        ### START CODE HERE ### (6 lines for linear, 5 lines for batch norm)         \n",
        "        layer_sizes=[26,256,128]\n",
        "        layer_sizes1=[256,128]\n",
        "        self.fc= nn.ModuleList([nn.Linear(layer_sizes[i-1],layer_sizes[i]) for i in range(1,len(layer_sizes))])\n",
        "        self.bn= nn.ModuleList([nn.BatchNorm1d(layer_sizes1[i]) for i in range(0,len(layer_sizes1))])\n",
        "        self.dropout=nn.Dropout(0.3)\n",
        "        ### END CODE HERE ###\n",
        "        \n",
        "        \n",
        "        # Initialize all layers\n",
        "        ### START CODE HERE ### (4 lines) \n",
        "        for m in self.modules():\n",
        "          if isinstance(m,nn.Linear):\n",
        "            nn.init.kaiming_normal_(m.weight)\n",
        "            nn.init.zeros_(m.bias)\n",
        "        ### END CODE HERE ###\n",
        "\n",
        "                        \n",
        "    def forward(self, x):\n",
        "        ### START CODE HERE ### \n",
        "        x= x.view(x.size(0),-1)\n",
        "        for i in range(0,len(self.fc)):\n",
        "          x=torch.relu(self.dropout(self.bn[i](self.fc[i](x))))\n",
        "\n",
        "        #x= self.fc[-1](x)\n",
        "        #x=torch.sigmoid(x)\n",
        "\n",
        "        # (7 to 18 lines - 1 line to flatten input, 6 lines for linear, 5 lines for bn, 6 lines for relu)  \n",
        "        ### END CODE HERE ###\n",
        "        \n",
        "        return x\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zy8lOvm4hbgc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyEnsemble(nn.Module):\n",
        "    def __init__(self, modelA, modelB,modelC):\n",
        "        super(MyEnsemble, self).__init__()\n",
        "        self.modelA = modelA\n",
        "        self.modelB = modelB\n",
        "        self.modelC=  modelC\n",
        "        layer_sizes=[133,128,64,5]\n",
        "        layer_sizes1=[128,64]\n",
        "        self.fc= nn.ModuleList([nn.Linear(layer_sizes[i-1],layer_sizes[i]) for i in range(1,len(layer_sizes))])\n",
        "        self.bn= nn.ModuleList([nn.BatchNorm1d(layer_sizes1[i]) for i in range(0,len(layer_sizes1))])\n",
        "        self.dropout=nn.Dropout(0.3)\n",
        "        \n",
        "    def forward(self, x1, x2):\n",
        "        x1 = self.modelC(self.modelB(x1))\n",
        "        x2 = self.modelA(x2)\n",
        "        x = torch.cat((x1, x2), dim=1)\n",
        "        x= x.view(x.size(0),-1)\n",
        "        for i in range(0,len(self.fc)-1):\n",
        "          x=torch.relu(self.dropout(self.bn[i](self.fc[i](x))))\n",
        "\n",
        "        x= self.fc[-1](x)\n",
        "        x=torch.sigmoid(x)\n",
        "\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVoyQXVVLJRG",
        "colab_type": "text"
      },
      "source": [
        "###Import helper function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EWVyAiE4hVHl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CombineDataset(data.Dataset):\n",
        "    \"\"\"Face Landmarks dataset.\"\"\"\n",
        "\n",
        "    def __init__(self, csv_file, root_dir,frame, transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            csv_file (string): Path to the csv file with annotations.\n",
        "            root_dir (string): Directory with all the images.\n",
        "            transform (callable, optional): Optional transform to be applied\n",
        "                on a sample.\n",
        "        \"\"\"\n",
        "        self.landmarks_frame = pd.read_csv(csv_file)\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.frame=frame\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.landmarks_frame)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "        X = []\n",
        "        for i in range(1,16): \n",
        "          img_name = os.path.join(self.root_dir, self.landmarks_frame.iloc[idx, 0])\n",
        "          image = Image.open(str(img_name)+'_'+str(i)+'.jpg')\n",
        "\n",
        "          if self.transform is not None:\n",
        "                image = self.transform(image)\n",
        "          X.append(image)\n",
        "\n",
        "        X = torch.stack(X, dim=0)\n",
        "        landmarks = self.landmarks_frame.iloc[idx, 27:]\n",
        "        landmarks = np.array(landmarks)\n",
        "        landmarks = landmarks.astype('float').reshape(-1, 5)\n",
        "        audio =self.landmarks_frame.iloc[idx, 1:27]\n",
        "        audio = np.array(audio)\n",
        "        audio = audio.astype('float').reshape(-1, 26)\n",
        "        #image = image.transpose((2, 0, 1))\n",
        "\n",
        "        return X,landmarks,audio"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4pYdpWOuKbOY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, device, train_loader, optimizer, epochs):\n",
        "    # set model as training mode\n",
        "    model.train()\n",
        "    model.to(device)\n",
        "    criterion=torch.nn.L1Loss()\n",
        "    \n",
        "    loss_over_time = [] # to track the loss as the network trains\n",
        "    average_over_time=[]\n",
        "    valid_accuracy=[]\n",
        "    valid_loss=[]\n",
        "    for epoch in range(epochs):\n",
        "      N_count = 0   # counting total trained sample in one epoch\n",
        "      running_loss = 0.0\n",
        "      running_corrects=0\n",
        "      sum_per_epoch=0\n",
        "      running_loss_per_epoch = 0.0\n",
        "      item=0\n",
        "      running_corrects_per_epoch=0\n",
        "      model.train()\n",
        "      for batch_idx, (X, y,data1) in enumerate(train_loader):\n",
        "          # distribute data to device\n",
        "          X, y = X.to(device), y.to(device).view(y.size(0), -1)\n",
        "          print(X.shape)\n",
        "          print(data1.shape)\n",
        "          data1 = data1.type(torch.cuda.FloatTensor)\n",
        "          data1 = data1.to(device)\n",
        "\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "          output =model(x1=X,x2=data1)   # output has dim = (batch, number of classes)\n",
        "\n",
        "          loss = criterion(output, y)\n",
        "          running_loss+=loss.item()\n",
        "\n",
        "          # to compute accuracy\n",
        "            # y_pred != output\n",
        "          output_pts=output.cpu()\n",
        "          key_pts=y.cpu()\n",
        "          running_corrects += torch.mean(1-abs(output_pts.data-key_pts))\n",
        "          #step_score = accuracy_score(y.cpu().data.squeeze().numpy(), output.cpu().data.squeeze().numpy())\n",
        "\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "          # show information\n",
        "          if batch_idx % 10 == 9:    # print every 10 batches\n",
        "                running_corrects_per_epoch+=running_corrects/10\n",
        "                running_loss_per_epoch+=running_loss/10\n",
        "                print('Epoch: {}, Batch: {}, Avg. Loss: {}, average accuracy: {}'.format(epoch + 1, batch_idx+1, running_loss/10,running_corrects/10))\n",
        "                running_loss = 0.0\n",
        "                running_corrects=0\n",
        "                sum_per_epoch+=1\n",
        "      print('Epoch: {}, Avg. Loss: {}, average accuracy: {}'.format(epoch + 1, running_loss_per_epoch/sum_per_epoch,running_corrects_per_epoch/sum_per_epoch))\n",
        "      loss_over_time.append(running_loss_per_epoch*10/sum_per_epoch)\n",
        "      average_over_time.append(running_corrects_per_epoch*10/sum_per_epoch)\n",
        "      torch.save(model.state_dict(), \"model_ensemble\"+str(epoch)+\".pt\")\n",
        "      acc,loss=evaluate(model)\n",
        "      print(\"evaluated\")\n",
        "      valid_accuracy.append(acc)\n",
        "      valid_loss.append(loss)\n",
        "\n",
        "\n",
        "    return loss_over_time, average_over_time,valid_loss,valid_accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4OLSMaLpKb_n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(net, use_gpu=True):\n",
        "    \n",
        "    # set to evaluation mode\n",
        "    net.eval()\n",
        "    criterion=torch.nn.L1Loss()\n",
        "    running_corrects = 0\n",
        "    item=0\n",
        "    accuracy_0=0\n",
        "    accuracy_1=0\n",
        "    accuracy_2=0\n",
        "    accuracy_3=0\n",
        "    accuracy_4=0\n",
        "    valid_loss=[]\n",
        "\n",
        "    for i, (X, y,data1) in enumerate(testloader):\n",
        "\n",
        "            X, y = X.to(device), y.to(device).view(y.size(0), -1)\n",
        "            data1 = data1.type(torch.cuda.FloatTensor)\n",
        "            data1 = data1.to(device)\n",
        "            with torch.no_grad():\n",
        "                output =net(x1=X,x2=data1)\n",
        "\n",
        "                item+=1\n",
        "                output=output.cpu()\n",
        "                y=y.cpu()\n",
        "                loss = criterion(output, y)\n",
        "                valid_loss.append(loss)\n",
        "                z=1-abs(output.data-y)\n",
        "                accuracy_0+=torch.mean(z[:,0])\n",
        "                accuracy_1+=torch.mean(z[:,1])\n",
        "                accuracy_2+=torch.mean(z[:,2])\n",
        "                accuracy_3+=torch.mean(z[:,3])\n",
        "                accuracy_4+=torch.mean(z[:,4])\n",
        "                \n",
        "                running_corrects += torch.mean(1-abs(output.data-y))\n",
        "  \n",
        "                \n",
        "                \n",
        "\n",
        "\n",
        "\n",
        "    print('Accuracy = {:.2f}%'.format(100*running_corrects/item))\n",
        "    print(\"Accuracy first item: \"+str(accuracy_0/item))\n",
        "    print(\"Accuracy second item: \"+str(accuracy_1/item))\n",
        "    print(\"Accuracy third item: \"+str(accuracy_2/item))\n",
        "    print(\"Accuracy fourth item: \"+str(accuracy_3/item))\n",
        "    print(\"Accuracy fifth item: \"+str(accuracy_4/item))\n",
        "    valid_losses = np.average(valid_loss)\n",
        "    return(running_corrects/item,valid_losses)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQif2YDILFgW",
        "colab_type": "text"
      },
      "source": [
        "###Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wonRUJ4bK8MA",
        "colab_type": "text"
      },
      "source": [
        "Dataloader for testing and training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GtaBf6pVmAtf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "dataset = CombineDataset(csv_file='/content/drive/My Drive/audio_validation.csv',\n",
        "                                    root_dir='/content/ImageData_validation/validation_15',frame=7,\n",
        "                                    transform = transforms.Compose([\n",
        "                                transforms.ToTensor(),\n",
        "                                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])]))\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "testloader = DataLoader(dataset, batch_size=15,\n",
        "                        shuffle=True, num_workers=8)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LxjzqMyIhhYs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = CombineDataset(csv_file='/content/drive/My Drive/audui_training_15.csv',\n",
        "                                    root_dir='/content/ImageData_training/training_15frame',\n",
        "                                    root_dir2='/content/ImageData_training-part3/training_15frame',\n",
        "                                    frame=15,\n",
        "                                    transform = transforms.Compose([\n",
        "                                transforms.ToTensor(),\n",
        "                                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])]))\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jgLLfBZbhiiE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainloader = DataLoader(dataset, batch_size=15,\n",
        "                        shuffle=True, num_workers=8)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ec_gNDD5Km0i",
        "colab_type": "text"
      },
      "source": [
        "Build model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TgAaO8NpKusG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model=MyResnet()\n",
        "_ = model.load_state_dict(torch.load(\"/content/drive/My Drive/My_ResNet15_frame (1).pt\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DGl1aVR6Kq6H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "cnn_encoder = ResCNNEncoder(model,fc_hidden1=CNN_fc_hidden1, fc_hidden2=CNN_fc_hidden2, drop_p=0.5, CNN_embed_dim=CNN_embed_dim).to(device)\n",
        "rnn_decoder = DecoderRNN(CNN_embed_dim=CNN_embed_dim, h_RNN_layers=RNN_hidden_layers, h_RNN=RNN_hidden_nodes, \n",
        "                         h_FC_dim=RNN_FC_dim, drop_p=0.5, num_classes=k).to(device)\n",
        "PATH= \"/content/drive/My Drive/MyResnetLSTMOnly(New_RNN)_20epochcnn_encoder.pt\"\n",
        "PATH2=\"/content/drive/My Drive/MyResnetOnlyLSTM(New_RNN)_20epoch_rnn_decoder.pt\"\n",
        "cnn_encoder.load_state_dict(torch.load(PATH))\n",
        "rnn_decoder.load_state_dict(torch.load(PATH2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AAkBdGWnKyfn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_audio=Audio()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NEjBUOtJhnbc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_ensemble=MyEnsemble(modelA=model_audio,modelB=cnn_encoder,modelC=rnn_decoder)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BtiyQMTQKENJ",
        "colab_type": "text"
      },
      "source": [
        "Freeze layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_lAtmOuBiJrm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for param in cnn_encoder.parameters():\n",
        "    param.requires_grad = False\n",
        "for param in rnn_decoder.parameters():\n",
        "    param.requires_grad = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uPvlkb05iKW8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = torch.optim.Adam(model_ensemble.parameters(), lr=0.001, weight_decay=1e-5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2O5s5mJwpqNV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hist_loss_model_ensemble,hist_corrects_model_ensemble,hist_loss_evaluate_model_ensemble,hist_corrects_evaluate_model_ensemble=train(model=model_ensemble,device=device,train_loader=trainloader,optimizer=optimizer,epochs=10)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}